{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX4n9TsbGw-f"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "0nbI5DtDGw-i"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnJztDZGw-n"
   },
   "source": [
    "# Text classification with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfN3bMR5Gw-o"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUWearf0Gw-p"
   },
   "source": [
    "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2VQo4bajwUU"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z682XYsrjkY9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rXHa-w9JZhb"
   },
   "source": [
    "Import `matplotlib` and create a helper function to plot graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mp1Z7P9pYRSK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VgpW3427KLx"
   },
   "source": [
    "\n",
    "\n",
    "# Get Datasets and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "SHRwRoP2nVHX",
    "outputId": "18c272f6-a315-46bf-f83a-8b8d9b075932"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a6923a6ff3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtestingDatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.2/test/dialogues_00'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows can only be passed if lines=True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         ):\n\u001b[0;32m--> 659\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m    660\u001b[0m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"method\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         return IOArgs(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mIncompleteRead\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdetect\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "testingDatasets = []\n",
    "trainingDatasets = []\n",
    "\n",
    "for i in range(2):\n",
    "  testingDatasets.append(pd.read_json('https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.2/test/dialogues_00' + str(i+1) + '.json'))\n",
    "\n",
    "for i in range(9):\n",
    "  trainingDatasets.append(pd.read_json('https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.2/train/dialogues_00' + str(i+1) + '.json'))\n",
    "\n",
    "for i in range(8):\n",
    "  trainingDatasets.append(pd.read_json('https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.2/train/dialogues_0' + str(i+10) + '.json'))\n",
    "\n",
    "testingDatasets = pd.concat([testingDatasets[0], testingDatasets[1]])\n",
    "trainingDatasets = pd.concat([trainingDatasets[0], trainingDatasets[1],trainingDatasets[2],trainingDatasets[3],trainingDatasets[4],trainingDatasets[5],trainingDatasets[6],trainingDatasets[7],trainingDatasets[8],trainingDatasets[9],trainingDatasets[10],trainingDatasets[11],trainingDatasets[12],trainingDatasets[13],trainingDatasets[14],trainingDatasets[15],trainingDatasets[16]])\n",
    "testingDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nWuRHtKxjcD_",
    "outputId": "e7101e52-3ba4-435a-f466-37caf35df21b"
   },
   "outputs": [],
   "source": [
    "trainingDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "XXw_AA8XEcUg",
    "outputId": "10be768a-20f2-4bf4-bdf7-9ba6ef4fd4bb"
   },
   "outputs": [],
   "source": [
    "# seleciona no json os campos relevantes. Apenas as falas dos usuários (ignora os do System)\n",
    "def getDataframes(dataframe):\n",
    "  entries_classifier = []\n",
    "  entries_extractor = []\n",
    "\n",
    "  for turn in dataframe['turns']:\n",
    "    for entry in turn:\n",
    "      if entry['speaker'] == 'USER':\n",
    "        intent = ''\n",
    "        slot_values = []\n",
    "        requested_slots = []\n",
    "        phrase = entry['utterance']\n",
    "        for frame in entry['frames']:  \n",
    "          if frame['state']['active_intent'] != 'NONE':\n",
    "            intent = frame['state']['active_intent']\n",
    "            slot_values = frame['state']['slot_values']\n",
    "            requested_slots = frame['state']['requested_slots']\n",
    "        entries_classifier.append({'entry': phrase, 'intent': intent})\n",
    "        entries_extractor.append({'entry': phrase, 'intent': intent, 'slot_values': slot_values, 'requested_slots': requested_slots})\n",
    "\n",
    "  classifier = pd.DataFrame.from_dict(entries_classifier)\n",
    "  extractor = pd.DataFrame.from_dict(entries_extractor)\n",
    "\n",
    "  return classifier, extractor\n",
    "\n",
    "test_classifier, test_extractor = getDataframes(testingDatasets)\n",
    "train_classifier, train_extractor = getDataframes(trainingDatasets)\n",
    "\n",
    "test_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingDatasets['turns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYkxo4l37mhx"
   },
   "source": [
    "# Classify Intents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIs1Wrnm8Ur9"
   },
   "source": [
    "## Recognize intents from other domains/services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umDgv_lk8S4b",
    "outputId": "cb6a699d-5a33-4dea-b212-f52d9fcde847"
   },
   "outputs": [],
   "source": [
    "# aqueles intents que não interessam ao projeto são marcados como \"Invalid\"\n",
    "def recognizeIntents(intent):\n",
    "  if (intent == 'find_restaurant' or intent == 'book_restaurant'):\n",
    "    return intent\n",
    "  return 'Invalid'\n",
    "\n",
    "train_classifier['intent'] = train_classifier['intent'].apply(recognizeIntents)\n",
    "test_classifier['intent'] = test_classifier['intent'].apply(recognizeIntents)\n",
    "train_extractor['intent'] = train_extractor['intent'].apply(recognizeIntents)\n",
    "test_extractor['intent'] = test_extractor['intent'].apply(recognizeIntents)\n",
    "train_classifier['intent'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYWzu_pBQKyP",
    "outputId": "069e24c0-7613-4e26-cb0c-e69751e60344"
   },
   "outputs": [],
   "source": [
    "test_classifier['intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ne2EcRxXiSU1",
    "outputId": "1fe122fa-d5dd-48da-835c-77836acbf0c1"
   },
   "outputs": [],
   "source": [
    "train_extractor['intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQL323K8iTId",
    "outputId": "48bd72ad-2b81-460d-e429-05eb0b9f7ace"
   },
   "outputs": [],
   "source": [
    "test_extractor['intent'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEsUzfa4RMCL"
   },
   "source": [
    "## Create categories for intents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9kg5OHkRPdn",
    "outputId": "2bb52349-c54e-40f1-d3b1-c88162bce9cf"
   },
   "outputs": [],
   "source": [
    "# transforma as strings categóricas em números, permitindo processar\n",
    "train_classifier['intent'] = pd.Categorical(train_classifier['intent'])\n",
    "train_classifier['intent'] = train_classifier.intent.cat.codes\n",
    "train_classifier['intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYzsq5HvRQzn",
    "outputId": "b97a4f95-9b9e-4aa0-97e5-b4d993bea6c2"
   },
   "outputs": [],
   "source": [
    "test_classifier['intent'] = pd.Categorical(test_classifier['intent'])\n",
    "test_classifier['intent'] = test_classifier.intent.cat.codes\n",
    "test_classifier['intent'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUCvlyS5-cvc"
   },
   "source": [
    "## Setup input pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXp2MQ9YDnbK",
    "outputId": "2c84c14a-5bc9-43f5-a6ce-9e10649b98ab"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_classifier['entry'], train_classifier['intent']))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_classifier['entry'], test_classifier['intent']))\n",
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDsCaZCDYZgm"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VznrltNOnUc5"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahNzycL1S4rN",
    "outputId": "96b72eb2-5ce8-424e-dd83-e967e439963d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# agora, ele já está com a batch de 64 entradas\n",
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5eWCo88voPY"
   },
   "source": [
    "## Create the text encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFevcItw15P_"
   },
   "source": [
    "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
    "\n",
    "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uC25Lu1Yvuqy"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "# pegando nossos textos e criando o vocabulário\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuQzVBbe3Ldu"
   },
   "source": [
    "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBoyjjWg0Ac9",
    "outputId": "f26ce6d4-0954-46b3-e43d-b4df6fa29a69"
   },
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjId5pua3jHQ"
   },
   "source": [
    "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGc7C9WiwRWs",
    "outputId": "938f20f8-3dd7-4d9e-dc34-b6358dc6e337"
   },
   "outputs": [],
   "source": [
    "# pegamos primeiros três de \"example\",\n",
    "# textos extraídos do train_dataset,\n",
    "# para servir de exemplo\n",
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como se pode ver, cada número representa uma palavra\n",
    "example.numpy()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5cjz0bS39IN"
   },
   "source": [
    "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
    "\n",
    "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
    "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_tD0QY5wXaK",
    "outputId": "a697633a-42d4-4f1e-a297-a59b4bd7825d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", example[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjUqGVBxGw-t"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7zsmInBOCPO"
   },
   "source": [
    "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgs6nnSTGw-t"
   },
   "source": [
    "Above is a diagram of the model. \n",
    "\n",
    "1. This model can be build as a `tf.keras.Sequential`.\n",
    "\n",
    "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
    "\n",
    "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
    "\n",
    "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
    "\n",
    "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
    "\n",
    "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
    "\n",
    "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
    "\n",
    "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
    "\n",
    "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4fodCI7soQi"
   },
   "source": [
    "The code to implement this is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwfoBkmRYcP3",
    "outputId": "dbeb2972-0c0c-4070-b18f-863fcd02aa32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIGmIGkkouUb"
   },
   "source": [
    "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF-PsCk1LwjY"
   },
   "source": [
    "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87a8-CwfKebw",
    "outputId": "21d22dcf-88ec-41eb-aeb9-e91e04147699"
   },
   "outputs": [],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRI776ZcH3Tf"
   },
   "source": [
    "Compile the Keras model to configure the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kj2xei41YZjC"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIwH3nto596k"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hw86wWS4YgR2",
    "outputId": "9b9e241d-9372-4dbe-a1e8-e3b87cd0f751"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaNbXi43YgUT",
    "outputId": "cac73187-9f14-44d4-9136-56cb8b46dada"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "OZmwt_mzaQJk",
    "outputId": "0859be41-5d74-4c13-efbd-f90afc34cd94"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xySAqjZJdJw0",
    "outputId": "96a93b5c-aa38-4bcd-81b2-08cddfe586c3"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(['i need a place to dine in the center thats expensive'])\n",
    "c = np.array(['Invalid', 'find_restaurant', 'book_restaurant'])\n",
    "print(predictions[0])\n",
    "print(c[np.argmax(predictions[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We3zyQTuhVDs"
   },
   "source": [
    "# Extract entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XfzOaR_iujv"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHyTX5_6ixD3",
    "outputId": "5609e1e4-6b86-4bdc-dc9e-37b90a431ca9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#   \"restaurant-area\",\n",
    "#   \"restaurant-bookday\",\n",
    "#   \"restaurant-bookpeople\",\n",
    "#   \"restaurant-booktime\",\n",
    "#   \"restaurant-food\",\n",
    "#   \"restaurant-name\",\n",
    "#   \"restaurant-pricerange\",\n",
    "test_extractor['slot_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "AGklBK4Yiz7R",
    "outputId": "47cf4380-f573-4959-853c-707dfc0a5324"
   },
   "outputs": [],
   "source": [
    "train_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "1mQ-pYBJi9IF",
    "outputId": "8f8d020c-43d9-47d8-bbb4-8c2deb3c7f7e"
   },
   "outputs": [],
   "source": [
    "# limpando tudo que não é relativo ao restaurante\n",
    "train_extractor = train_extractor[train_extractor['intent']!='Invalid']\n",
    "test_extractor = test_extractor[test_extractor['intent']!='Invalid']\n",
    "test_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjVMOZhti-Wg",
    "outputId": "3fa04e02-71dd-41b0-df6f-4837ddfc3817"
   },
   "outputs": [],
   "source": [
    "train_extractor['slot_values'][16] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqrCMEi4fcSE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "timepat = re.compile(\"\\d{1,2}[:]\\d{1,2}\")\n",
    "pricepat = re.compile(\"\\d{1,3}[.]\\d{1,2}\")\n",
    "\n",
    "replacements = []\n",
    "\n",
    "def insertSpace(token, text):\n",
    "    sidx = 0\n",
    "    while True:\n",
    "        sidx = text.find(token, sidx)\n",
    "        if sidx == -1:\n",
    "            break\n",
    "        if sidx + 1 < len(text) and re.match('[0-9]', text[sidx - 1]) and \\\n",
    "                re.match('[0-9]', text[sidx + 1]):\n",
    "            sidx += 1\n",
    "            continue\n",
    "        if text[sidx - 1] != ' ':\n",
    "            text = text[:sidx] + ' ' + text[sidx:]\n",
    "            sidx += 1\n",
    "        if sidx + len(token) < len(text) and text[sidx + len(token)] != ' ':\n",
    "            text = text[:sidx + 1] + ' ' + text[sidx + 1:]\n",
    "        sidx += 1\n",
    "    return text\n",
    "\n",
    "def normalize(text):\n",
    "    # lower case every word\n",
    "    text = text.lower()\n",
    "\n",
    "    # replace white spaces in front and end\n",
    "    text = re.sub(r'^\\s*|\\s*$', '', text)\n",
    "\n",
    "    # hotel domain pfb30\n",
    "    text = re.sub(r\"b&b\", \"bed and breakfast\", text)\n",
    "    text = re.sub(r\"b and b\", \"bed and breakfast\", text)\n",
    "\n",
    "    # normalize phone number\n",
    "    ms = re.findall('\\(?(\\d{3})\\)?[-.\\s]?(\\d{3})[-.\\s]?(\\d{4,5})', text)\n",
    "    if ms:\n",
    "        sidx = 0\n",
    "        for m in ms:\n",
    "            sidx = text.find(m[0], sidx)\n",
    "            if text[sidx - 1] == '(':\n",
    "                sidx -= 1\n",
    "            eidx = text.find(m[-1], sidx) + len(m[-1])\n",
    "            text = text.replace(text[sidx:eidx], ''.join(m))\n",
    "\n",
    "    # normalize postcode\n",
    "    ms = re.findall('([a-z]{1}[\\. ]?[a-z]{1}[\\. ]?\\d{1,2}[, ]+\\d{1}[\\. ]?[a-z]{1}[\\. ]?[a-z]{1}|[a-z]{2}\\d{2}[a-z]{2})',\n",
    "                    text)\n",
    "    if ms:\n",
    "        sidx = 0\n",
    "        for m in ms:\n",
    "            sidx = text.find(m, sidx)\n",
    "            eidx = sidx + len(m)\n",
    "            text = text[:sidx] + re.sub('[,\\. ]', '', m) + text[eidx:]\n",
    "\n",
    "    # weird unicode bug\n",
    "    text = re.sub(u\"(\\u2018|\\u2019)\", \"'\", text)\n",
    "\n",
    "    value_time_list = re.findall(timepat, text)\n",
    "    value_price_list = re.findall(pricepat, text)\n",
    "\n",
    "    # replace time and and price\n",
    "    text = re.sub(timepat, ' [value_time] ', text)\n",
    "    text = re.sub(pricepat, ' [value_price] ', text)\n",
    "    #text = re.sub(pricepat2, '[value_price]', text)\n",
    "\n",
    "    # replace st.\n",
    "    text = text.replace(';', ',')\n",
    "    text = re.sub('$\\/', '', text)\n",
    "    text = text.replace('/', ' and ')\n",
    "\n",
    "    # replace other special characters\n",
    "    text = text.replace('-', ' ')\n",
    "    text = re.sub('[\\\":\\<>@\\(\\)]', '', text)\n",
    "\n",
    "    # insert white space before and after tokens:\n",
    "    for token in ['?', '.', ',', '!']:\n",
    "        text = insertSpace(token, text)\n",
    "\n",
    "    # insert white space for 's\n",
    "    text = insertSpace('\\'s', text)\n",
    "\n",
    "    # replace it's, does't, you'd ... etc\n",
    "    text = re.sub('^\\'', '', text)\n",
    "    text = re.sub('\\'$', '', text)\n",
    "    text = re.sub('\\'\\s', ' ', text)\n",
    "    text = re.sub('\\s\\'', ' ', text)\n",
    "    for fromx, tox in replacements:\n",
    "        text = ' ' + text + ' '\n",
    "        text = text.replace(fromx, tox)[1:-1]\n",
    "\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    # concatenate numbers\n",
    "    tmp = text\n",
    "    tokens = text.split()\n",
    "    i = 1\n",
    "    while i < len(tokens):\n",
    "        if re.match(u'^\\d+$', tokens[i]) and \\\n",
    "                re.match(u'\\d+$', tokens[i - 1]):\n",
    "            tokens[i - 1] += tokens[i]\n",
    "            del tokens[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    #return time values\n",
    "    time_iterator = 0\n",
    "\n",
    "    while (time_iterator < len(value_time_list)):\n",
    "      if (len(value_time_list[time_iterator]) < 5):\n",
    "        value_time_list[time_iterator] = '0'+ value_time_list[time_iterator]\n",
    "      time_iterator += 1\n",
    "    \n",
    "    word_list = text.split()\n",
    "    word_iterator = 0\n",
    "    \n",
    "    vtl = value_time_list[:]\n",
    "    vpl = value_price_list[:]\n",
    "\n",
    "    while len(value_time_list) != 0:\n",
    "      if (word_list[word_iterator] == '[value_time]'):\n",
    "        word_list[word_iterator] = value_time_list[0]\n",
    "        del value_time_list[0]\n",
    "      \n",
    "      word_iterator += 1\n",
    "#     print(word_list)\n",
    "\n",
    "\n",
    "    #return price values\n",
    "    word_iterator = 0\n",
    "    while len(value_price_list) != 0:\n",
    "      if (word_list[word_iterator] == '[value_price]'):\n",
    "        word_list[word_iterator] = value_price_list[0]\n",
    "        del value_price_list[0]\n",
    "      \n",
    "      word_iterator += 1\n",
    "        \n",
    "    return ' '.join(word_list), vpl, vtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lqd4SUK_lfKw",
    "outputId": "4268bcf3-1e56-4449-fbb7-7187ab82fc9b"
   },
   "outputs": [],
   "source": [
    "n = normalize(\"Yes, I do. I'll need it booked for the same day, same people, and we'd like to eat at 19:00, with a of 10.00.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67rvkOIXoPjj"
   },
   "outputs": [],
   "source": [
    "def has_compound_word(values): #return compound words in slot values (pricerange values, area, bookday...)\n",
    "  keys = []\n",
    "  for key, val in values.items():\n",
    "    for unit in val:\n",
    "      if (' ' in unit):\n",
    "        keys.append(key)\n",
    "  \n",
    "  return list(set(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing data\n",
    "all_chars = []\n",
    "all_text = ' '.join([i for i in train_extractor['entry']]).lower()\n",
    "# all_chars.append([i for i in all_text])\n",
    "for i in range(len(all_text)):\n",
    "    all_chars.append(all_text[i])\n",
    "all_chars = np.unique(all_chars)\n",
    "\n",
    "char_to_int = dict((c, i) for i, c in enumerate(all_chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(all_chars))\n",
    "\n",
    "# todos os chars de todos os textos\n",
    "n_chars = len(all_text)\n",
    "# caracteres únicos\n",
    "n_vocab = len(all_chars)\n",
    "\n",
    "print(\"Todos os chars:\", n_chars, \"\\n\")\n",
    "print(\"Vocabulario:\", n_vocab, \"\\n\")\n",
    "print(\"Char to int\\n\", char_to_int, \"\\n\")\n",
    "print(\"Int to char\\n\", int_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepares dataset where the input is sequence of 100 characters and target is next character.\n",
    "seq_length = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    #  uma espécie de janela que vai movendo de um em um,\n",
    "    #  sempre pegando 100 chars (seq_in) e depois o char seguinte (seq_out)\n",
    "    seq_in = all_text[i:i + seq_length]\n",
    "    seq_out = all_text[i + seq_length]\n",
    "    # o dataX vai ficar com listas de 100 em 100 chars\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    # já o dataY, vai ficar com uma lista de chars    \n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# X = array de 100 em 100, sendo eles numeros representando cada um dos chars\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# one hot encodes the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X[0]))\n",
    "print(len(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passando dados para o modelo\n",
    "# Mapeando cada palavra pra 100 um vetor de 100\n",
    "# Usamos uma dropout layer para prevenir overfitting\n",
    "\n",
    "embedding_dim =100\n",
    "max_length =100\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs = 20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFAMZgoCfGMU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, valid_test_data = train_test_split(data_formatted,test_size=0.30, random_state=42)\n",
    "valid_data, test_data = train_test_split(valid_test_data,test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3fMTOBGfNhW",
    "outputId": "d4642460-d038-44f9-aa38-7997624f5832"
   },
   "outputs": [],
   "source": [
    "# training generator\n",
    "def gen_train_series():\n",
    "\n",
    "    for eg in train_data:\n",
    "        yield eg[0],eg[1]\n",
    "\n",
    "# validation generator\n",
    "def gen_valid_series():\n",
    "\n",
    "    for eg in valid_data:\n",
    "        yield eg[0],eg[1]\n",
    "\n",
    "# test generator\n",
    "def gen_test_series():\n",
    "\n",
    "    for eg in test_data:\n",
    "        yield eg[0],eg[1]\n",
    "  \n",
    "# create Dataset objects for train, test and validation sets  \n",
    "series = tf.data.Dataset.from_generator(gen_train_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "series_valid = tf.data.Dataset.from_generator(gen_valid_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "series_test = tf.data.Dataset.from_generator(gen_test_series,output_types=(tf.int32, tf.int32),output_shapes = ((None, None)))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE=1000\n",
    "\n",
    "# create padded batch series objects for train, test and validation sets\n",
    "ds_series_batch = series.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
    "ds_series_batch_valid = series_valid.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
    "ds_series_batch_test = series_test.padded_batch(BATCH_SIZE, padded_shapes=([None], [None]), drop_remainder=True)\n",
    "\n",
    "# print example batches\n",
    "for input_example_batch, target_example_batch in ds_series_batch_valid.take(1):\n",
    "    print(input_example_batch)\n",
    "    print(target_example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nGqRS7MfPcL",
    "outputId": "285504db-dcd2-4477-c905-76a84f2d1314"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)+1\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "label_size = len(labels)  \n",
    "\n",
    "# build LSTM model\n",
    "def build_model(vocab_size,label_size, embedding_dim, rnn_units, batch_size):\n",
    "      model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                            batch_input_shape=[batch_size, None],mask_zero=True),\n",
    "          tf.keras.layers.LSTM(rnn_units,\n",
    "                      return_sequences=True,\n",
    "                      stateful=True,\n",
    "                      recurrent_initializer='glorot_uniform'),\n",
    "          tf.keras.layers.Dense(label_size)\n",
    "          ])\n",
    "      return model\n",
    "\n",
    "model = build_model(\n",
    "      vocab_size = len(vocab)+1,\n",
    "      label_size=len(labels)+1,\n",
    "      embedding_dim=embedding_dim,\n",
    "      rnn_units=rnn_units,\n",
    "      batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgxI5pJ_fRKM",
    "outputId": "fda87c09-0606-46bc-c5b2-9e4973366ff2"
   },
   "outputs": [],
   "source": [
    "print(vocab_size, label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCIijDUEfT90"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss,metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1piln7UcfYcM",
    "outputId": "24c6a21b-7455-4495-d782-4773a844dea3"
   },
   "outputs": [],
   "source": [
    "history = model.fit(ds_series_batch, epochs=10, validation_data=ds_series_batch_valid,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjaXYEiWfcXq",
    "outputId": "4554fca4-f2f4-4deb-e603-0b1ac60bac4b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "preds = np.array([])\n",
    "y_trues= np.array([])\n",
    "\n",
    "# iterate through test set, make predictions based on trained model\n",
    "for input_example_batch, target_example_batch in ds_series_batch_test:\n",
    "\n",
    "  pred=model.predict_on_batch(input_example_batch)\n",
    "  pred_max=tf.argmax(tf.nn.softmax(pred),2).numpy().flatten()\n",
    "  y_true=target_example_batch.numpy().flatten()\n",
    "\n",
    "  preds=np.concatenate([preds,pred_max])\n",
    "  y_trues=np.concatenate([y_trues,y_true])\n",
    "\n",
    "# remove padding from evaluation\n",
    "remove_padding = [(p,y) for p,y in zip(preds,y_trues) if y!=0]\n",
    "\n",
    "r_p = [x[0] for x in remove_padding]\n",
    "r_t = [x[1] for x in remove_padding]\n",
    "\n",
    "# print confusion matrix and classification report\n",
    "print(confusion_matrix(r_p,r_t))\n",
    "print(classification_report(r_p,r_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix(r_p,r_t), index = ['0', 'O', 'restaurant-area', 'restaurant-bookday',\n",
    "       'restaurant-bookpeople', 'restaurant-booktime', 'restaurant-food',\n",
    "       'restaurant-name', 'restaurant-pricerange'], columns = ['0', 'O', 'restaurant-area', 'restaurant-bookday',\n",
    "       'restaurant-bookpeople', 'restaurant-booktime', 'restaurant-food',\n",
    "       'restaurant-name', 'restaurant-pricerange'])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "_tDckAjnfePw",
    "outputId": "1b1fecfc-9be4-411e-f66a-90462cae142f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Projeto PLN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
